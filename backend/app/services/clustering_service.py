"""
K-means/HDBSCAN ã‚’ä½¿ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import KMeans, HDBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import uuid
from datetime import datetime
from sqlalchemy.orm import Session

from app.models.vector import (
    SuccessConversationVector,
    ClusterResult,
    ClusterAssignment,
    ClusterRepresentative
)


logger = logging.getLogger(__name__)


class ClusteringService:
    """æˆåŠŸä¼šè©±ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, db: Session):
        self.db = db
    
    async def perform_clustering(
        self,
        algorithm: str = "kmeans",
        k_range: Tuple[int, int] = (2, 15),
        auto_select_k: bool = True,
        clustering_params: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æˆåŠŸä¼šè©±ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        
        Args:
            algorithm: 'kmeans' or 'hdbscan'
            k_range: K-meansã®å ´åˆã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ç¯„å›²
            auto_select_k: æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®è‡ªå‹•æ±ºå®š
            clustering_params: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            {
                'cluster_result_id': 'uuid',
                'algorithm': 'kmeans',
                'cluster_count': 5,
                'silhouette_score': 0.65,
                'cluster_assignments': [...],
                'cluster_centroids': [...],
                'performance_metrics': {...}
            }
        """
        try:
            # 1. æˆåŠŸä¼šè©±ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            vectors_data = self._get_success_vectors()
            if len(vectors_data) < 2:
                raise ValueError("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã¯æœ€ä½2ã¤ã®æˆåŠŸä¼šè©±ãƒ™ã‚¯ãƒˆãƒ«ãŒå¿…è¦ã§ã™")
            
            vectors = np.array([item['embedding'] for item in vectors_data])
            vector_ids = [item['id'] for item in vectors_data]
            
            logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡: {len(vectors)}ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«")
            
            # 2. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¿œã˜ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            if algorithm == "kmeans":
                clustering_result = await self._perform_kmeans_clustering(
                    vectors, k_range, auto_select_k, clustering_params
                )
            elif algorithm == "hdbscan":
                clustering_result = await self._perform_hdbscan_clustering(
                    vectors, clustering_params
                )
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {algorithm}")
            
            # 3. çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            cluster_result_id = await self._save_clustering_result(
                algorithm=algorithm,
                cluster_count=clustering_result['cluster_count'],
                parameters=clustering_result['parameters'],
                silhouette_score=clustering_result['silhouette_score'],
                labels=clustering_result['labels'],
                vector_ids=vector_ids,
                centroids=clustering_result.get('centroids')
            )
            
            return {
                'cluster_result_id': str(cluster_result_id),
                'algorithm': algorithm,
                'cluster_count': clustering_result['cluster_count'],
                'silhouette_score': clustering_result['silhouette_score'],
                'cluster_assignments': clustering_result['assignments'],
                'cluster_centroids': clustering_result.get('centroids'),
                'performance_metrics': clustering_result['performance_metrics']
            }
            
        except Exception as e:
            logger.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _get_success_vectors(self) -> List[Dict[str, Any]]:
        """æˆåŠŸä¼šè©±ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
        query = self.db.query(SuccessConversationVector).filter(
            SuccessConversationVector.is_success == True
        ).all()
        
        return [
            {
                'id': str(vector.id),
                'session_id': str(vector.session_id),
                'embedding': vector.embedding,
                'text': vector.chunk_text
            }
            for vector in query
        ]
    
    async def _perform_kmeans_clustering(
        self,
        vectors: np.ndarray,
        k_range: Tuple[int, int],
        auto_select_k: bool,
        params: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        default_params = {
            'n_init': 10,
            'max_iter': 300,
            'random_state': 42
        }
        if params:
            default_params.update(params)
        
        best_k = k_range[0]
        best_score = -1
        best_model = None
        scores_by_k = {}
        
        if auto_select_k:
            # æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¢ç´¢
            logger.info(f"æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ¢ç´¢ä¸­: {k_range[0]}-{k_range[1]}")
            
            for k in range(k_range[0], min(k_range[1] + 1, len(vectors))):
                kmeans = KMeans(n_clusters=k, **default_params)
                labels = kmeans.fit_predict(vectors)
                
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°ã§è©•ä¾¡
                if len(set(labels)) > 1:  # ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã®ã¿
                    silhouette_avg = float(silhouette_score(vectors, labels))
                    scores_by_k[k] = silhouette_avg
                    
                    if silhouette_avg > best_score:
                        best_score = silhouette_avg
                        best_k = k
                        best_model = kmeans
                        
                    logger.info(f"K={k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°={silhouette_avg:.3f}")
        else:
            # æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°ã§å®Ÿè¡Œ
            best_k = k_range[0]
            best_model = KMeans(n_clusters=best_k, **default_params)
            labels = best_model.fit_predict(vectors)
            best_score = silhouette_score(vectors, labels) if len(set(labels)) > 1 else 0
        
        # æœ€çµ‚çš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
        final_labels = best_model.fit_predict(vectors)
        centroids = best_model.cluster_centers_
        
        # å„ãƒ™ã‚¯ãƒˆãƒ«ã®é‡å¿ƒã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—
        distances_to_centroids = []
        for i, (vector, label) in enumerate(zip(vectors, final_labels)):
            label = int(label)  # numpyã‚¹ã‚«ãƒ©ãƒ¼ã‚’intã«å¤‰æ›
            if label >= 0:  # æœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹ã‚¿
                centroid = centroids[label]
                distance = np.linalg.norm(vector - centroid)
                distances_to_centroids.append(distance)
            else:
                distances_to_centroids.append(float('inf'))
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        performance_metrics = {
            'silhouette_score': best_score,
            'inertia': best_model.inertia_,
            'n_iter': best_model.n_iter_,
            'scores_by_k': scores_by_k
        }
        
        if len(set(final_labels)) > 1:
            performance_metrics['calinski_harabasz_score'] = calinski_harabasz_score(
                vectors, final_labels
            )
        
        # ã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦æƒ…å ±
        assignments = []
        for i, (label, distance) in enumerate(zip(final_labels, distances_to_centroids)):
            assignments.append({
                'vector_index': i,
                'cluster_label': int(label),
                'distance_to_centroid': float(distance)
            })
        
        return {
            'cluster_count': best_k,
            'labels': final_labels.tolist(),
            'centroids': centroids.tolist(),
            'silhouette_score': best_score,
            'parameters': default_params,
            'assignments': assignments,
            'performance_metrics': performance_metrics
        }
    
    async def _perform_hdbscan_clustering(
        self,
        vectors: np.ndarray,
        params: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        default_params = {
            'min_cluster_size': max(5, len(vectors) // 10),  # å‹•çš„ã«èª¿æ•´
            'min_samples': 3,
            'metric': 'cosine',
            'cluster_selection_epsilon': 0.0,
            'alpha': 1.0
        }
        if params:
            default_params.update(params)
        
        logger.info(f"HDBSCANå®Ÿè¡Œä¸­: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿={default_params}")
        
        # HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        clusterer = HDBSCAN(**default_params)
        cluster_labels = clusterer.fit_predict(vectors)
        
        # ãƒã‚¤ã‚ºï¼ˆ-1ãƒ©ãƒ™ãƒ«ï¼‰ã‚’é™¤ã„ãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºã‚’é™¤ãï¼‰
        silhouette_avg = 0.0
        if n_clusters > 1:
            valid_indices = cluster_labels >= 0
            if np.sum(valid_indices) > 1:
                silhouette_avg = float(silhouette_score(
                    vectors[valid_indices], 
                    cluster_labels[valid_indices]
                ))
        
        # å„ãƒ™ã‚¯ãƒˆãƒ«ã®æ‰€å±ã‚¯ãƒ©ã‚¹ã‚¿é‡å¿ƒã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—
        distances_to_centroids = []
        cluster_centroids = {}
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®é‡å¿ƒã‚’è¨ˆç®—
        for label in unique_labels:
            if int(label) >= 0:  # ãƒã‚¤ã‚ºã§ã¯ãªã„
                cluster_vectors = vectors[cluster_labels == label]
                centroid = np.mean(cluster_vectors, axis=0)
                cluster_centroids[label] = centroid
        
        # å„ãƒ™ã‚¯ãƒˆãƒ«ã®é‡å¿ƒã‹ã‚‰ã®è·é›¢
        for i, label in enumerate(cluster_labels):
            label = int(label)  # numpyã‚¹ã‚«ãƒ©ãƒ¼ã‚’intã«å¤‰æ›
            if label >= 0 and label in cluster_centroids:
                distance = np.linalg.norm(vectors[i] - cluster_centroids[label])
                distances_to_centroids.append(distance)
            else:
                distances_to_centroids.append(float('inf'))  # ãƒã‚¤ã‚º
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        performance_metrics = {
            'silhouette_score': silhouette_avg,
            'n_clusters': n_clusters,
            'n_noise': np.sum(cluster_labels == -1),
            'cluster_persistence': clusterer.cluster_persistence_.tolist() if hasattr(clusterer, 'cluster_persistence_') else None
        }
        
        # ã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦æƒ…å ±
        assignments = []
        for i, (label, distance) in enumerate(zip(cluster_labels, distances_to_centroids)):
            assignments.append({
                'vector_index': i,
                'cluster_label': int(label),
                'distance_to_centroid': float(distance)
            })
        
        return {
            'cluster_count': n_clusters,
            'labels': cluster_labels.tolist(),
            'centroids': [centroid.tolist() for centroid in cluster_centroids.values()],
            'silhouette_score': silhouette_avg,
            'parameters': default_params,
            'assignments': assignments,
            'performance_metrics': performance_metrics
        }
    
    async def _save_clustering_result(
        self,
        algorithm: str,
        cluster_count: int,
        parameters: Dict[str, Any],
        silhouette_score: float,
        labels: List[int],
        vector_ids: List[str],
        centroids: Optional[List[List[float]]] = None
    ) -> uuid.UUID:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        
        try:
            logger.info(f"ğŸ’¾ Saving clustering result - algorithm: {algorithm}, clusters: {cluster_count}")
            logger.info(f"ğŸ“Š Labels type: {type(labels)}, length: {len(labels) if hasattr(labels, '__len__') else 'no length'}")
            logger.info(f"ğŸ“‹ Labels preview: {labels[:5] if len(labels) >= 5 else labels}")
            logger.info(f"ğŸ¯ Centroids type: {type(centroids)}, shape: {centroids.shape if hasattr(centroids, 'shape') else 'no shape'}")
            # ClusterResultã‚’ä¿å­˜
            cluster_result = ClusterResult(
                algorithm=algorithm,
                cluster_count=cluster_count,
                parameters=parameters,
                silhouette_score=silhouette_score
            )
            self.db.add(cluster_result)
            self.db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚
            
            # ClusterAssignmentã‚’ä¸€æ‹¬ä¿å­˜
            logger.info(f"ğŸ”„ Processing {len(labels)} cluster assignments")
            assignments = []
            for i, (vector_id, label) in enumerate(zip(vector_ids, labels)):
                logger.debug(f"Processing assignment {i}: vector_id={vector_id}, label={label} (type: {type(label)})")
                # numpyã‚¹ã‚«ãƒ©ãƒ¼ã‚’Pythonã®intã«å¤‰æ›
                label = int(label)
                
                # é‡å¿ƒã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—ï¼ˆcentroids ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                distance_to_centroid = None
                logger.debug(f"Checking centroids: centroids={centroids is not None}, label={label}, label >= 0: {label >= 0}")
                if centroids is not None and label >= 0 and label < len(centroids):
                    vector_embedding = self.db.query(SuccessConversationVector.embedding).filter(
                        SuccessConversationVector.id == vector_id
                    ).scalar()
                    
                    if vector_embedding is not None:
                        centroid = np.array(centroids[label])
                        vector_np = np.array(vector_embedding)
                        distance_to_centroid = float(np.linalg.norm(vector_np - centroid))
                
                assignment = ClusterAssignment(
                    vector_id=vector_id,
                    cluster_result_id=cluster_result.id,
                    cluster_label=label,
                    distance_to_centroid=distance_to_centroid
                )
                assignments.append(assignment)
            
            self.db.add_all(assignments)
            self.db.commit()
            
            logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœä¿å­˜å®Œäº†: {cluster_result.id}")
            return cluster_result.id
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise


class OptimalClustersDetector:
    """æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    @staticmethod
    def elbow_method(vectors: np.ndarray, k_range: Tuple[int, int]) -> Dict[str, Any]:
        """ã‚¨ãƒ«ãƒœãƒ¼æ³•ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®š"""
        inertias = []
        k_values = list(range(k_range[0], min(k_range[1] + 1, len(vectors))))
        
        for k in k_values:
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(vectors)
            inertias.append(kmeans.inertia_)
        
        # ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è‡ªå‹•æ¤œå‡º
        if len(inertias) >= 3:
            # äºŒæ¬¡å¾®åˆ†ã‚’è¨ˆç®—ã—ã¦ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡º
            second_derivatives = []
            for i in range(1, len(inertias) - 1):
                second_deriv = inertias[i-1] - 2*inertias[i] + inertias[i+1]
                second_derivatives.append(second_deriv)
            
            # æœ€å¤§ã®äºŒæ¬¡å¾®åˆ†ã‚’æŒã¤ç‚¹ã‚’ã‚¨ãƒ«ãƒœãƒ¼ã¨ã™ã‚‹
            if second_derivatives:
                elbow_index = second_derivatives.index(max(second_derivatives)) + 1
                optimal_k = k_values[elbow_index]
            else:
                optimal_k = k_values[len(k_values)//2]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            optimal_k = k_values[0]
        
        return {
            'optimal_k': optimal_k,
            'k_values': k_values,
            'inertias': inertias,
            'second_derivatives': second_derivatives if len(inertias) >= 3 else []
        }
    
    @staticmethod
    def gap_statistic(vectors: np.ndarray, k_range: Tuple[int, int], n_refs: int = 10) -> Dict[str, Any]:
        """Gapçµ±è¨ˆã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®š"""
        from sklearn.cluster import KMeans
        import random
        
        gaps = []
        k_values = list(range(k_range[0], min(k_range[1] + 1, len(vectors))))
        
        for k in k_values:
            # å®Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(vectors)
            real_inertia = kmeans.inertia_
            
            # å‚ç…§ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            ref_inertias = []
            for _ in range(n_refs):
                # å®Ÿãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
                min_vals = vectors.min(axis=0)
                max_vals = vectors.max(axis=0)
                random_data = np.random.uniform(min_vals, max_vals, vectors.shape)
                
                ref_kmeans = KMeans(n_clusters=k, random_state=random.randint(0, 1000))
                ref_kmeans.fit(random_data)
                ref_inertias.append(ref_kmeans.inertia_)
            
            # Gapå€¤è¨ˆç®—
            mean_ref_inertia = np.mean(ref_inertias)
            gap = np.log(mean_ref_inertia) - np.log(real_inertia)
            gaps.append(gap)
        
        # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã¯æœ€å¤§ã®Gapå€¤ã‚’æŒã¤k
        optimal_k = k_values[gaps.index(max(gaps))]
        
        return {
            'optimal_k': optimal_k,
            'k_values': k_values,
            'gaps': gaps
        }


# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def create_clustering_service(vector_db: Session) -> ClusteringService:
    """ClusteringServiceã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°"""
    return ClusteringService(vector_db)